{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1567596,"sourceType":"datasetVersion","datasetId":926321},{"sourceId":3394100,"sourceType":"datasetVersion","datasetId":2042641},{"sourceId":7882043,"sourceType":"datasetVersion","datasetId":4575686}],"dockerImageVersionId":30018,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os, math, sys\nimport time, datetime\nimport glob, itertools\nimport argparse, random\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import datasets\nfrom torch.autograd import Variable\nfrom torchvision.models import vgg19\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision.utils import save_image, make_grid\n\nimport plotly\nfrom scipy import signal\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport matplotlib.pyplot as plt\n\nfrom PIL import Image\nfrom tqdm import tqdm_notebook as tqdm\nfrom sklearn.model_selection import train_test_split\n\nrandom.seed(42)\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2024-03-19T13:15:05.797207Z","iopub.execute_input":"2024-03-19T13:15:05.797590Z","iopub.status.idle":"2024-03-19T13:15:10.045571Z","shell.execute_reply.started":"2024-03-19T13:15:05.797558Z","shell.execute_reply":"2024-03-19T13:15:10.044825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Settings","metadata":{}},{"cell_type":"code","source":"# path to pre-trained models\n# pretrained_model_path = \"../input/cyclegan-translating-horses-zebras-pytorch/saved_models\"\n\n# epoch to start training from\nepoch_start = 0\n# number of epochs of training\nn_epochs = 10\n\n# name of the dataset\n# dataset_path = \"../input/horse2zebra-dataset\"\n\n# size of the batches\"\nbatch_size = 4\n# adam: learning rate\nlr = 0.02\n# adam: decay of first order momentum of gradient\nb1 = 0.5\n# adam: decay of first order momentum of gradient\nb2 = 0.999\n# epoch from which to start lr decay\ndecay_epoch = 1\n# number of cpu threads to use during batch generation\nn_workers = 16\n# size of image height\nimg_height = 256\n# size of image width\nimg_width = 256\n# number of image channels\nchannels = 1\n# interval between saving generator outputs\nsample_interval = 100\n# interval between saving model checkpoints\ncheckpoint_interval = -1\n# number of residual blocks in generator\nn_residual_blocks = 9\n# cycle loss weight\nlambda_cyc = 10.0\n# identity loss weight\nlambda_id = 5.0\n# Development / Debug Mode\ndebug_mode = False\n\n# Create images and checkpoint directories\nos.makedirs(\"images\", exist_ok=True)\nos.makedirs(\"saved_models\", exist_ok=True)","metadata":{"execution":{"iopub.status.busy":"2024-03-19T05:54:33.978202Z","iopub.execute_input":"2024-03-19T05:54:33.978514Z","iopub.status.idle":"2024-03-19T05:54:33.987160Z","shell.execute_reply.started":"2024-03-19T05:54:33.978485Z","shell.execute_reply":"2024-03-19T05:54:33.986251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Define Utilities","metadata":{}},{"cell_type":"code","source":"\nclass ReplayBuffer:\n    def __init__(self, max_size=50):\n        assert max_size > 0\n        self.max_size = max_size\n        self.data = []\n\n    def push_and_pop(self, data):\n        to_return = []\n        for element in data.data:\n            element = torch.unsqueeze(element, 0)\n            if len(self.data) < self.max_size:\n                self.data.append(element)\n                to_return.append(element)\n            else:\n                if random.uniform(0, 1) > 0.5:\n                    i = random.randint(0, self.max_size - 1)\n                    to_return.append(self.data[i].clone())\n                    self.data[i] = element\n                else:\n                    to_return.append(element)\n        return Variable(torch.cat(to_return))\n","metadata":{"execution":{"iopub.status.busy":"2024-03-19T05:54:33.988444Z","iopub.execute_input":"2024-03-19T05:54:33.988779Z","iopub.status.idle":"2024-03-19T05:54:33.999124Z","shell.execute_reply.started":"2024-03-19T05:54:33.988750Z","shell.execute_reply":"2024-03-19T05:54:33.998296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Define Dataset Class","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ImageDataset(Dataset):\n    def __init__(self, root, transforms_=None, unaligned=False, mode=\"train\"):\n        self.transform = transforms.Compose(transforms_)\n        self.unaligned = unaligned\n\n        self.files_A = sorted(glob.glob(os.path.join(root, f\"{mode}A\") + \"/*.*\"))\n        self.files_B = sorted(glob.glob(os.path.join(root, f\"{mode}B\") + \"/*.*\"))\n        if debug_mode:\n            self.files_A = self.files_A[:100]\n            self.files_B = self.files_B[:100]\n\n    def __getitem__(self, index):\n        image_A = Image.open(self.files_A[index % len(self.files_A)])\n\n        if self.unaligned:\n            image_B = Image.open(self.files_B[random.randint(0, len(self.files_B) - 1)])\n        else:\n            image_B = Image.open(self.files_B[index % len(self.files_B)])\n\n        # Convert grayscale images to rgb\n#         if image_A.mode != \"RGB\":\n#             image_A = to_rgb(image_A)\n#         if image_B.mode != \"RGB\":\n#             image_B = to_rgb(image_B)\n\n        item_A = self.transform(image_A)\n        item_B = self.transform(image_B)\n        return {\"A\": item_A, \"B\": item_B}\n\n    def __len__(self):\n        return max(len(self.files_A), len(self.files_B))","metadata":{"execution":{"iopub.status.busy":"2024-03-19T05:54:34.000374Z","iopub.execute_input":"2024-03-19T05:54:34.000689Z","iopub.status.idle":"2024-03-19T05:54:34.014603Z","shell.execute_reply.started":"2024-03-19T05:54:34.000662Z","shell.execute_reply":"2024-03-19T05:54:34.013788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Get Train/Test Dataloaders","metadata":{}},{"cell_type":"code","source":"# pip install split-folders","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport shutil\nimport numpy as np\nimport pandas as pd\npath=\"/kaggle/working/\"\nos.makedirs(os.path.join(path,'data'),exist_ok=True)\n\npath='/kaggle/working/data'\nos.makedirs(os.path.join(path,'trainA'),exist_ok=True)\nos.makedirs(os.path.join(path,'trainB'),exist_ok=True)\n\n\ndef organize_folders(root_folder, dest_folder):\n    for split_folder in ['test', 'train', 'val']:\n        split_path = os.path.join(root_folder, split_folder)\n\n        for subfolder in os.listdir(split_path):\n            subfolder_path = os.path.join(split_path, subfolder)\n            dest_path = os.path.join(dest_folder, subfolder)\n\n            for image_file in os.listdir(subfolder_path):\n                source_path = os.path.join(subfolder_path, image_file)\n\n                if 'bg' in image_file:\n                    destination_folder = 'trainA'\n#                     continue\n                elif 'cl' in image_file:\n#                     destination_folder = 'trainA'\n                    continue\n                elif 'nm' in image_file:\n                    destination_folder = 'trainB'\n#                 print(image_file)\n                # Constructing the destination path using os.path.join\n                destination_path = os.path.join(dest_folder, destination_folder, os.path.basename(image_file))\n                shutil.copy(source_path, destination_path)\n\n# Example usage\nroot_folder = \"/kaggle/input/casiab-identification/DATASETidentification\"\ndest_folder = \"/kaggle/working/data/\"\norganize_folders(root_folder, dest_folder)\n\n\n# splitfolders.ratio(input_folder, output=output_folder, seed=1337, ratio=ratio, group_prefix=None)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-19T05:54:34.017068Z","iopub.execute_input":"2024-03-19T05:54:34.017360Z","iopub.status.idle":"2024-03-19T05:55:55.826077Z","shell.execute_reply.started":"2024-03-19T05:54:34.017331Z","shell.execute_reply":"2024-03-19T05:55:55.825247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import shutil\nimport random\n\n\nos.makedirs(\"/kaggle/working/data/testA\", exist_ok=True)\nos.makedirs(\"/kaggle/working/data/testB\", exist_ok=True)\n\nimages=os.listdir(\"/kaggle/working/data/trainA\")\n\nimg=random.sample(images,815)\n# img=random.sample(images,1631)\n\nfor image in img:\n    src = os.path.join(\"/kaggle/working/data/trainA\", image)\n    dst = os.path.join(\"/kaggle/working/data/testA\", image)\n    shutil.move(src, dst)\n\n\nimages=os.listdir(\"/kaggle/working/data/trainB\")\nimg=random.sample(images,2446)\nfor image in img:\n    src = os.path.join(\"/kaggle/working/data/trainB\", image)\n    dst = os.path.join(\"/kaggle/working/data/testB\", image)\n    shutil.move(src, dst)\n\n\n     ","metadata":{"execution":{"iopub.status.busy":"2024-03-19T05:55:55.827479Z","iopub.execute_input":"2024-03-19T05:55:55.827766Z","iopub.status.idle":"2024-03-19T05:55:55.957876Z","shell.execute_reply.started":"2024-03-19T05:55:55.827739Z","shell.execute_reply":"2024-03-19T05:55:55.956889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"images=os.listdir(\"/kaggle/working/data/trainA\")\nlen(images)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-18T19:03:12.567100Z","iopub.execute_input":"2024-03-18T19:03:12.567396Z","iopub.status.idle":"2024-03-18T19:03:12.577990Z","shell.execute_reply.started":"2024-03-18T19:03:12.567367Z","shell.execute_reply":"2024-03-18T19:03:12.577103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Image transformations\ntransforms_ = [\n    transforms.Resize(int(img_height * 1.12), Image.BICUBIC),\n    transforms.RandomCrop((img_height, img_width)),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5,), (0.5,)),\n\n]\n\n\n\n# Training data loader\ntrain_dataloader = DataLoader(\n    ImageDataset(\"/kaggle/working/data/\", transforms_=transforms_, unaligned=True),\n    batch_size=batch_size,\n    shuffle=True,\n    num_workers=n_workers,\n)\n# Test data loader\ntest_dataloader = DataLoader(\n    ImageDataset(\"/kaggle/working/data/\", transforms_=transforms_, unaligned=True, mode=\"test\"),\n    batch_size=1,\n    shuffle=True,\n    num_workers=1,\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-19T05:55:55.959448Z","iopub.execute_input":"2024-03-19T05:55:55.959861Z","iopub.status.idle":"2024-03-19T05:55:56.023841Z","shell.execute_reply.started":"2024-03-19T05:55:55.959821Z","shell.execute_reply":"2024-03-19T05:55:56.022683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image = next(iter(train_dataloader))\nprint(image.get('A').shape,image.get('B').shape)","metadata":{"execution":{"iopub.status.busy":"2024-03-18T18:04:37.922548Z","iopub.execute_input":"2024-03-18T18:04:37.922916Z","iopub.status.idle":"2024-03-18T18:04:38.426904Z","shell.execute_reply.started":"2024-03-18T18:04:37.922878Z","shell.execute_reply":"2024-03-18T18:04:38.425889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Define Model Classes","metadata":{}},{"cell_type":"code","source":"def weights_init_normal(m):\n    classname = m.__class__.__name__\n    if classname.find(\"Conv\") != -1:\n        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n        if hasattr(m, \"bias\") and m.bias is not None:\n            torch.nn.init.constant_(m.bias.data, 0.0)\n    elif classname.find(\"BatchNorm2d\") != -1:\n        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n        torch.nn.init.constant_(m.bias.data, 0.0)\n        \n        \n# class SelfAttention(nn.Module):\n#     def __init__(self, in_channels):\n#         super(SelfAttention, self).__init__()\n#         self.query_conv = nn.Conv2d(in_channels, in_channels // 8, 1)\n#         self.key_conv = nn.Conv2d(in_channels, in_channels // 8, 1)\n#         self.value_conv = nn.Conv2d(in_channels, in_channels, 1)\n#         self.gamma = nn.Parameter(torch.zeros(1))\n\n#     def forward(self, x):\n#         batch_size, channels, height, width = x.size()\n#         proj_query = self.query_conv(x).view(batch_size, -1, width * height).permute(0, 2, 1)\n#         proj_key = self.key_conv(x).view(batch_size, -1, width * height)\n#         energy = torch.bmm(proj_query, proj_key)\n#         attention = F.softmax(energy, dim=-1)\n#         proj_value = self.value_conv(x).view(batch_size, -1, width * height)\n#         out = torch.bmm(proj_value, attention.permute(0, 2, 1))\n#         out = out.view(batch_size, channels, height, width)\n#         out = self.gamma * out + x\n#         return out\n\n    \n# class ResidualBlock(nn.Module):\n#     def __init__(self, in_features):\n#         super(ResidualBlock, self).__init__()\n#         self.block = nn.Sequential(\n#             nn.ReflectionPad2d(1),\n#             nn.Conv2d(in_features, in_features, 3),\n#             nn.BatchNorm2d(in_features),\n#             nn.ReLU(inplace=True),\n#             SelfAttention(in_features),  # Add self-attention mechanism\n#             nn.ReflectionPad2d(1),\n#             nn.Conv2d(in_features, in_features, 3),\n#             nn.BatchNorm2d(in_features),\n#         )\n\n#     def forward(self, x):\n#         return x + self.block(x)\n\nclass ResidualBlock(nn.Module):\n    def __init__(self, in_features):\n        super(ResidualBlock, self).__init__()\n\n        self.block = nn.Sequential(\n            nn.ReflectionPad2d(1),\n            nn.Conv2d(in_features, in_features, 3),\n            nn.BatchNorm2d(in_features),\n            nn.ReLU(inplace=True),\n            nn.ReflectionPad2d(1),\n            nn.Conv2d(in_features, in_features, 3),\n            nn.BatchNorm2d(in_features),\n        )\n\n    def forward(self, x):\n        return x + self.block(x)\n\n\nclass GeneratorResNet(nn.Module):\n    def __init__(self, input_shape, num_residual_blocks):\n        super(GeneratorResNet, self).__init__()\n\n        channels = input_shape[0]\n\n        # Initial convolution block\n        out_features = 64\n        model = [\n            nn.ReflectionPad2d(3),\n            nn.Conv2d(channels, out_features, 7),\n            nn.BatchNorm2d(out_features),\n            nn.ReLU(inplace=True),\n        ]\n        in_features = out_features\n\n        # Downsampling\n        for _ in range(2):\n            out_features *= 2\n            model += [\n                nn.Conv2d(in_features, out_features, 3, stride=2, padding=1),\n                nn.BatchNorm2d(out_features),\n                nn.ReLU(inplace=True),\n            ]\n            in_features = out_features\n\n        # Residual blocks\n        for _ in range(num_residual_blocks):\n            model += [ResidualBlock(out_features)]\n\n        # Upsampling\n        for _ in range(2):\n            out_features //= 2\n            model += [\n                nn.Upsample(scale_factor=2),\n                nn.Conv2d(in_features, out_features, 3, stride=1, padding=1),\n                nn.BatchNorm2d(out_features),\n                nn.ReLU(inplace=True),\n            ]\n            in_features = out_features\n\n        # Output layer\n        model += [nn.ReflectionPad2d(3), nn.Conv2d(out_features, channels, 7), nn.Tanh()]\n\n        self.model = nn.Sequential(*model)\n\n    def forward(self, x):\n        return self.model(x)\n\n\nclass Discriminator(nn.Module):\n    def __init__(self, input_shape):\n        super(Discriminator, self).__init__()\n\n        channels, height, width = input_shape\n\n        # Calculate output shape of image discriminator (PatchGAN)\n        self.output_shape = (1, height // 2 ** 4, width // 2 ** 4)\n\n        def discriminator_block(in_filters, out_filters, normalize=True):\n            \"\"\"Returns downsampling layers of each discriminator block\"\"\"\n            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1)]\n            if normalize:\n                layers.append(nn.InstanceNorm2d(out_filters))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            return layers\n\n        self.model = nn.Sequential(\n            *discriminator_block(channels, 64, normalize=False),\n            *discriminator_block(64, 128),\n            *discriminator_block(128, 256),\n            *discriminator_block(256, 512),\n            nn.ZeroPad2d((1, 0, 1, 0)),\n            nn.Conv2d(512, 1, 4, padding=1)\n        )\n\n    def forward(self, img):\n        return self.model(img)","metadata":{"execution":{"iopub.status.busy":"2024-03-19T05:55:56.025811Z","iopub.execute_input":"2024-03-19T05:55:56.026214Z","iopub.status.idle":"2024-03-19T05:55:56.069319Z","shell.execute_reply.started":"2024-03-19T05:55:56.026170Z","shell.execute_reply":"2024-03-19T05:55:56.068407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"g=GeneratorResNet((1,256,256),9)\nd=Discriminator((1,256,256))\ndummy = torch.randn(1, 1, 256, 256)\nout_gen=g(dummy)\nout_disc=d(dummy)\nprint(out_gen.shape,out_disc.shape)","metadata":{"execution":{"iopub.status.busy":"2024-03-18T10:45:12.262917Z","iopub.execute_input":"2024-03-18T10:45:12.263370Z","iopub.status.idle":"2024-03-18T10:45:13.401855Z","shell.execute_reply.started":"2024-03-18T10:45:12.263321Z","shell.execute_reply":"2024-03-18T10:45:13.400927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Train CycleGAN","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport torchvision.transforms as transforms\ndef displayImage(x):\n    x=x[0].cpu()\n    image_bag = (x.permute(1,2,0).detach().numpy() * 255).astype('uint8')\n    plt.imshow(image_bag,cmap=\"gray\")\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Losses\n\ndef wgan_loss(real_output, fake_output):\n    return -torch.mean(real_output) + torch.mean(fake_output)\n\n\ncriterion_GAN = torch.nn.MSELoss()\ncriterion_cycle = torch.nn.L1Loss()\ncriterion_identity = torch.nn.L1Loss()\n\ncuda = torch.cuda.is_available()\n\ninput_shape = (channels, img_height, img_width)\n# Initialize generator and discriminator\nG_AB = GeneratorResNet(input_shape, n_residual_blocks)\nG_BA = GeneratorResNet(input_shape, n_residual_blocks)\nD_A = Discriminator(input_shape)\nD_B = Discriminator(input_shape)\n\nif cuda:\n    G_AB = G_AB.cuda()\n    G_BA = G_BA.cuda()\n    D_A = D_A.cuda()\n    D_B = D_B.cuda()\n    criterion_GAN.cuda()\n    criterion_cycle.cuda()\n    criterion_identity.cuda()\n\nif epoch_start != 0:\n    # Load pretrained models\n    G_AB.load_state_dict(torch.load(f\"{pretrained_model_path}/G_AB.pth\"))\n    G_BA.load_state_dict(torch.load(f\"{pretrained_model_path}/G_BA.pth\"))\n    D_A.load_state_dict(torch.load(f\"{pretrained_model_path}/D_A.pth\"))\n    D_B.load_state_dict(torch.load(f\"{pretrained_model_path}/D_B.pth\"))\nelse:\n    # Initialize weights\n    G_AB.apply(weights_init_normal)\n    G_BA.apply(weights_init_normal)\n    D_A.apply(weights_init_normal)\n    D_B.apply(weights_init_normal)\n\n# Optimizers\noptimizer_G = torch.optim.Adam(\n    itertools.chain(G_AB.parameters(), G_BA.parameters()), lr=lr, betas=(b1, b2)\n)\n\n# optimizer_G_AB = torch.optim.Adam(G_AB.parameters(), lr=lr, betas=(b1, b2))\n# optimizer_G_BA = torch.optim.Adam(G_BA.parameters(), lr=lr, betas=(b1, b2))\n\noptimizer_D_A = torch.optim.Adam(D_A.parameters(), lr=lr, betas=(b1, b2))\noptimizer_D_B = torch.optim.Adam(D_B.parameters(), lr=lr, betas=(b1, b2))\n\nTensor = torch.cuda.FloatTensor if cuda else torch.Tensor\n\n# Buffers of previously generated samples\nfake_A_buffer = ReplayBuffer()\nfake_B_buffer = ReplayBuffer()\n\ntrain_counter = []\ntrain_losses_gen, train_losses_id, train_losses_gan, train_losses_cyc = [], [], [], []\ntrain_losses_disc, train_losses_disc_a, train_losses_disc_b = [], [], []\n\ntest_counter = [2*idx*len(train_dataloader.dataset) for idx in range(epoch_start+1, n_epochs+1)]\ntest_losses_gen, test_losses_disc = [], []\n","metadata":{"execution":{"iopub.status.busy":"2024-03-19T05:55:56.070867Z","iopub.execute_input":"2024-03-19T05:55:56.071183Z","iopub.status.idle":"2024-03-19T05:56:00.334029Z","shell.execute_reply.started":"2024-03-19T05:55:56.071155Z","shell.execute_reply":"2024-03-19T05:56:00.333273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\ntorch.cuda.empty_cache()\n# Check available memory on CUDA device\ncuda_memory_stats = torch.cuda.memory_stats()\nfree_memory_bytes = cuda_memory_stats['allocated_bytes.all.peak'] - cuda_memory_stats['allocated_bytes.all.current']\nfree_memory_mb = free_memory_bytes / 1024 / 1024\nprint(\"Free CUDA memory:\", free_memory_mb, \"MB\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm_notebook as tqdm\n\ntorch.cuda.empty_cache()\nfor epoch in range(epoch_start, n_epochs):\n    #### Training\n    loss_gen = loss_id = loss_gan = loss_cyc = 0.0\n    loss_disc = loss_disc_a = loss_disc_b = 0.0\n    tqdm_bar = tqdm(train_dataloader, desc=f'Training Epoch {epoch} ', total=int(len(train_dataloader)))\n    for batch_idx, batch in enumerate(tqdm_bar):\n        # Set model input\n        real_A = Variable(batch[\"A\"].type(Tensor))\n        real_B = Variable(batch[\"B\"].type(Tensor))\n        # Adversarial ground truths\n        valid = Variable(Tensor(np.ones((real_A.size(0), *D_A.output_shape))), requires_grad=False)\n        fake = Variable(Tensor(np.zeros((real_A.size(0), *D_A.output_shape))), requires_grad=False)\n        \n\n        ### Train Generators\n        G_AB.train()\n        G_BA.train()\n        # Identity loss\n        loss_id_A = criterion_identity(G_BA(real_A), real_A)\n        loss_id_B = criterion_identity(G_AB(real_B), real_B)\n        loss_identity = (loss_id_A + loss_id_B) / 2\n        # GAN loss\n        fake_B = G_AB(real_A)\n        loss_GAN_AB = criterion_GAN(D_B(fake_B), valid)\n        fake_A = G_BA(real_B)\n        loss_GAN_BA = criterion_GAN(D_A(fake_A), valid)\n        loss_GAN = (loss_GAN_AB + loss_GAN_BA) / 2\n        # Cycle loss\n        recov_A = G_BA(fake_B)\n        loss_cycle_A = criterion_cycle(recov_A, real_A)\n        recov_B = G_AB(fake_A)\n        loss_cycle_B = criterion_cycle(recov_B, real_B)\n        loss_cycle = (loss_cycle_A + loss_cycle_B) / 2\n\n        # Total loss for each generator\n#         loss_G_AB = lambda_id * loss_id_A + loss_GAN_AB + lambda_cyc * loss_cycle_A\n#         loss_G_BA = lambda_id * loss_id_B + loss_GAN_BA + lambda_cyc * loss_cycle_B\n#         optimizer_G_AB.zero_grad()\n#         optimizer_G_BA.zero_grad()\n#         loss_G_AB.backward()\n#         loss_G_BA.backward()\n#         optimizer_G_AB.step()\n#         optimizer_G_BA.step()\n        \n        # Total loss\n        loss_G = lambda_id * loss_identity + loss_GAN + lambda_cyc * loss_cycle\n        loss_G.backward()\n        optimizer_G.step()\n        \n\n        ### Train Discriminator-A\n        D_A.train()\n        optimizer_D_A.zero_grad()\n        \n        # Real loss\n        loss_real = criterion_GAN(D_A(real_A), valid)\n#         loss_real = wgan_loss(D_A(real_A), valid)\n\n\n        # Fake loss (on batch of previously generated samples)\n        fake_A_ = fake_A_buffer.push_and_pop(fake_A)\n        \n        loss_fake = criterion_GAN(D_A(fake_A_.detach()), fake)\n#         loss_fake = wgan_loss(D_A(fake_A_.detach()), fake)\n\n        # Total loss\n        loss_D_A = (loss_real + loss_fake) / 2\n        loss_D_A.backward()\n        optimizer_D_A.step()\n\n        ### Train Discriminator-B\n        D_B.train()\n        optimizer_D_B.zero_grad()\n        # Real loss\n        loss_real = criterion_GAN(D_B(real_B), valid)\n#         loss_real = wgan_loss(D_B(real_B), valid)\n\n        # Fake loss (on batch of previously generated samples)\n        fake_B_ = fake_B_buffer.push_and_pop(fake_B)\n        loss_fake = criterion_GAN(D_B(fake_B_.detach()), fake)\n#         loss_fake = wgan_loss(D_B(fake_B_.detach()), fake)\n\n        # Total loss\n        loss_D_B = (loss_real + loss_fake) / 2\n        loss_D_B.backward()\n        optimizer_D_B.step()\n        loss_D = (loss_D_A + loss_D_B) / 2\n\n        ### Log Progress\n        loss_gen += loss_G.item(); loss_id += loss_identity.item(); loss_gan += loss_GAN.item(); loss_cyc += loss_cycle.item()\n        loss_disc += loss_D.item(); loss_disc_a += loss_D_A.item(); loss_disc_b += loss_D_B.item()\n        train_counter.append(2*(batch_idx*batch_size + real_A.size(0) + epoch*len(train_dataloader.dataset)))\n        train_losses_gen.append(loss_G.item()); train_losses_id.append(loss_identity.item()); train_losses_gan.append(loss_GAN.item()); train_losses_cyc.append(loss_cycle.item())\n        train_losses_disc.append(loss_D.item()); train_losses_disc_a.append(loss_D_A.item()); train_losses_disc_b.append(loss_D_B.item())\n        tqdm_bar.set_postfix(Gen_loss=loss_gen/(batch_idx+1), identity=loss_id/(batch_idx+1), adv=loss_gan/(batch_idx+1), cycle=loss_cyc/(batch_idx+1),\n                            Disc_loss=loss_disc/(batch_idx+1), disc_a=loss_disc_a/(batch_idx+1), disc_b=loss_disc_b/(batch_idx+1))\n\n    real_A = make_grid(real_A, nrow=1, normalize=True)\n    real_B = make_grid(real_B, nrow=1, normalize=True)\n    fake_A = make_grid(fake_A, nrow=1, normalize=True)\n    fake_B = make_grid(fake_B, nrow=1, normalize=True)\n    # Arange images along y-axis\n    image_grid = torch.cat((real_A, fake_B, real_B, fake_A), -1)\n\n    image_grid_np = image_grid.permute(1, 2, 0).cpu().detach().numpy()\n\n    # Display the image\n    plt.imshow(image_grid_np)\n    plt.axis('off')\n    plt.show()\n    torch.save(G_AB.state_dict(), \"saved_models/G_AB.pth\")","metadata":{"execution":{"iopub.status.busy":"2024-03-18T15:41:37.800957Z","iopub.execute_input":"2024-03-18T15:41:37.801327Z","iopub.status.idle":"2024-03-18T17:44:27.969571Z","shell.execute_reply.started":"2024-03-18T15:41:37.801292Z","shell.execute_reply":"2024-03-18T17:44:27.967746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Testing\nimport matplotlib.pyplot as plt\n\nloss_gen = loss_id = loss_gan = loss_cyc = 0.0\nloss_disc = loss_disc_a = loss_disc_b = 0.0\ntqdm_bar = tqdm(test_dataloader, desc=f'Testing epoch', total=int(len(test_dataloader)))\nfor batch_idx, batch in enumerate(tqdm_bar):\n    # Set model input\n    real_A = Variable(batch[\"A\"].type(Tensor))\n    real_B = Variable(batch[\"B\"].type(Tensor))\n    # Adversarial ground truths\n    valid = Variable(Tensor(np.ones((real_A.size(0), *D_A.output_shape))), requires_grad=False)\n    fake = Variable(Tensor(np.zeros((real_A.size(0), *D_A.output_shape))), requires_grad=False)\n\n    ### Test Generators\n    G_AB.eval()\n    G_BA.eval()\n    # Identity loss\n    loss_id_A = criterion_identity(G_BA(real_A), real_A)\n    loss_id_B = criterion_identity(G_AB(real_B), real_B)\n    loss_identity = (loss_id_A + loss_id_B) / 2\n    # GAN loss\n    fake_B = G_AB(real_A)\n    loss_GAN_AB = criterion_GAN(D_B(fake_B), valid)\n    fake_A = G_BA(real_B)\n    loss_GAN_BA = criterion_GAN(D_A(fake_A), valid)\n    loss_GAN = (loss_GAN_AB + loss_GAN_BA) / 2\n    # Cycle loss\n    recov_A = G_BA(fake_B)\n    loss_cycle_A = criterion_cycle(recov_A, real_A)\n    recov_B = G_AB(fake_A)\n    loss_cycle_B = criterion_cycle(recov_B, real_B)\n    loss_cycle = (loss_cycle_A + loss_cycle_B) / 2\n    # Total loss\n    loss_G = loss_GAN + lambda_cyc * loss_cycle + lambda_id * loss_identity\n\n    ### Test Discriminator-A\n    D_A.eval()\n    # Real loss\n    loss_real = criterion_GAN(D_A(real_A), valid)\n    # Fake loss (on batch of previously generated samples)\n    fake_A_ = fake_A_buffer.push_and_pop(fake_A)\n    loss_fake = criterion_GAN(D_A(fake_A_.detach()), fake)\n    # Total loss\n    loss_D_A = (loss_real + loss_fake) / 2\n\n    ### Test Discriminator-B\n    D_B.eval()\n    # Real loss\n    loss_real = criterion_GAN(D_B(real_B), valid)\n    # Fake loss (on batch of previously generated samples)\n    fake_B_ = fake_B_buffer.push_and_pop(fake_B)\n    loss_fake = criterion_GAN(D_B(fake_B_.detach()), fake)\n    # Total loss\n    loss_D_B = (loss_real + loss_fake) / 2\n    loss_D = (loss_D_A + loss_D_B) / 2\n\n    ### Log Progress\n    loss_gen += loss_G.item(); loss_id += loss_identity.item(); loss_gan += loss_GAN.item(); loss_cyc += loss_cycle.item()\n    loss_disc += loss_D.item(); loss_disc_a += loss_D_A.item(); loss_disc_b += loss_D_B.item()\n    tqdm_bar.set_postfix(Gen_loss=loss_gen/(batch_idx+1), identity=loss_id/(batch_idx+1), adv=loss_gan/(batch_idx+1), cycle=loss_cyc/(batch_idx+1),\n                        Disc_loss=loss_disc/(batch_idx+1), disc_a=loss_disc_a/(batch_idx+1), disc_b=loss_disc_b/(batch_idx+1))\n\n\n    # If at sample interval save image\n    if random.uniform(0,1)<0.4:\n        # Arrange images along x-axis\n        real_A = make_grid(real_A, nrow=1, normalize=True)\n        real_B = make_grid(real_B, nrow=1, normalize=True)\n        fake_A = make_grid(fake_A, nrow=1, normalize=True)\n        fake_B = make_grid(fake_B, nrow=1, normalize=True)\n        # Arange images along y-axis\n        image_grid = torch.cat((real_A, fake_B, real_B, fake_A), -1)\n        \n        image_grid_np = image_grid.permute(1, 2, 0).cpu().detach().numpy()\n    \n        # Display the image\n        plt.imshow(image_grid_np)\n        plt.axis('off')\n        plt.show()\n\ntest_losses_gen.append(loss_gen/len(test_dataloader))\ntest_losses_disc.append(loss_disc/len(test_dataloader))\n\n    # Save model checkpoints\nif np.argmin(test_losses_gen) == len(test_losses_gen)-1:\n    # Save model checkpoints\n    torch.save(G_AB.state_dict(), \"saved_models/G_AB.pth\")\n    torch.save(G_BA.state_dict(), \"saved_models/G_BA.pth\")\n    torch.save(D_A.state_dict(), \"saved_models/D_A.pth\")\n    torch.save(D_B.state_dict(), \"saved_models/D_B.pth\")","metadata":{"execution":{"iopub.status.busy":"2024-03-18T15:27:14.750902Z","iopub.execute_input":"2024-03-18T15:27:14.751212Z","iopub.status.idle":"2024-03-18T15:27:52.664507Z","shell.execute_reply.started":"2024-03-18T15:27:14.751180Z","shell.execute_reply":"2024-03-18T15:27:52.663251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#load weights to generator and discriminator\n\n# G_AB=torch.load('/kaggle/input/weight-files/G_AB.pth')\n# G_BA=torch.load('/kaggle/input/weight-files/G_BA.pth')\n# D_A=torch.load('/kaggle/input/weight-files/D_A.pth')\n# D_B=torch.load('/kaggle/input/weight-files/D_B.pth')\n\n\nmodel_path = '/kaggle/input/weight-files/G_AB (3).pth'\n\n# model_path='/kaggle/working/saved_models/G_AB.pth'\nmodel_state = torch.load(model_path, map_location=torch.device('cpu'))\nG_AB = GeneratorResNet(input_shape, n_residual_blocks)\nG_AB.load_state_dict(model_state)  \nG_AB.eval() \n\n# model_path = '/kaggle/input/weight-files/G_BA.pth'\n# model_state = torch.load(model_path, map_location=torch.device('cpu'))\n# G_BA = GeneratorResNet(input_shape, n_residual_blocks)\n# G_BA.load_state_dict(model_state)  \n# G_BA.eval() ","metadata":{"execution":{"iopub.status.busy":"2024-03-19T05:56:00.335261Z","iopub.execute_input":"2024-03-19T05:56:00.335625Z","iopub.status.idle":"2024-03-19T05:56:00.923880Z","shell.execute_reply.started":"2024-03-19T05:56:00.335587Z","shell.execute_reply":"2024-03-19T05:56:00.923005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nG_AB.cuda()\nG_AB.eval()\n\ntqdm_bar = tqdm(test_dataloader, desc=f'Testing epoch', total=int(len(test_dataloader)))\nfor batch_idx, batch in enumerate(tqdm_bar):\n    # Set model input\n    real_A = Variable(batch[\"A\"].type(Tensor)).cuda()\n    fake_B=G_AB(real_A).cuda()\n\n    real_A = make_grid(real_A, nrow=1, normalize=True)\n    fake_B = make_grid(fake_B, nrow=1, normalize=True)\n\n    image_grid = torch.cat((real_A, fake_B), -1)\n    \n    image_grid_np = image_grid.permute(1, 2, 0).cpu().detach().numpy()\n    \n    plt.imshow(image_grid_np)\n    plt.axis('off')\n    plt.show()\n    if(batch_idx==5):\n        break","metadata":{"execution":{"iopub.status.busy":"2024-03-19T05:56:00.925308Z","iopub.execute_input":"2024-03-19T05:56:00.925686Z","iopub.status.idle":"2024-03-19T05:56:02.392400Z","shell.execute_reply.started":"2024-03-19T05:56:00.925639Z","shell.execute_reply":"2024-03-19T05:56:02.391475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### CNN","metadata":{}},{"cell_type":"code","source":"# import torch\n# import torch.nn as nn\n# from torchvision import transforms\n# from PIL import Image\n# import matplotlib.pyplot as plt\n\n# # Define your model architecture\n# class CNNModel(nn.Module):\n#     def __init__(self):\n#         super(CNNModel, self).__init__()\n#         # Convolutional layers\n#         self.conv1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, stride=1, padding=1)\n#         self.conv2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n#         self.conv3 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1)\n#         self.conv4 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1)\n        \n#         # Maxpool layers\n#         self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n        \n#         # Fully connected layer\n#         self.fc = nn.Linear(512 * 15 * 15, 124)  # Adjust the input size based on the output size of the last convolutional layer\n        \n#         # Xavier Initialization\n#         self.initialize_weights()\n\n#     def forward(self, x):\n#         # Convolutional layers with ReLU activation and maxpooling\n#         x = F.relu(self.conv1(x))\n#         x = self.maxpool(x)\n#         x = F.relu(self.conv2(x))\n#         x = self.maxpool(x)\n#         x = F.relu(self.conv3(x))\n#         x = self.maxpool(x)\n#         x = F.relu(self.conv4(x))\n#         x = self.maxpool(x)\n        \n#         # Flatten the output for fully connected layers\n#         x = x.view(-1, 512 * 15 * 15)\n        \n#         # Fully connected layer\n#         x = self.fc(x)\n        \n#         return x\n    \n#     def initialize_weights(self):\n#         # Xavier Initialization for convolutional layers\n#         nn.init.xavier_uniform_(self.conv1.weight)\n#         nn.init.xavier_uniform_(self.conv2.weight)\n#         nn.init.xavier_uniform_(self.conv3.weight)\n#         nn.init.xavier_uniform_(self.conv4.weight)\n        \n#         # Xavier Initialization for fully connected layer\n#         nn.init.xavier_uniform_(self.fc.weight)\n\n# # Load the trained model\n# model_path = '/kaggle/input/weight-files/cnn-0.5.pth'\n# model_state = torch.load(model_path, map_location=torch.device('cpu'))\n# model = CNNModel()\n# model.load_state_dict(model_state)","metadata":{"execution":{"iopub.status.busy":"2024-03-18T19:00:54.249359Z","iopub.execute_input":"2024-03-18T19:00:54.249690Z","iopub.status.idle":"2024-03-18T19:00:54.254803Z","shell.execute_reply.started":"2024-03-18T19:00:54.249661Z","shell.execute_reply":"2024-03-18T19:00:54.253932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SelfAttention(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(SelfAttention, self).__init__()\n        self.query = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n        self.key = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n        self.value = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n        self.softmax = nn.Softmax(dim=-1)\n\n    def forward(self, x):\n        query = self.query(x)\n        key = self.key(x)\n        value = self.value(x)\n\n        energy = torch.matmul(query, key.transpose(2, 3))  # Calculate energy scores\n        attention_weights = self.softmax(energy)  # Apply softmax to get attention weights\n        attended_values = torch.matmul(attention_weights, value)  # Apply attention weights to values\n\n        return attended_values\n\n\nclass CNNModel(nn.Module):\n    def __init__(self):\n        super(CNNModel, self).__init__()\n        # Convolutional layers\n        self.conv1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, stride=1, padding=1)\n        self.conv2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n        self.conv3 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1)\n        self.conv4 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1)\n        \n        # Maxpool layers\n        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n        \n        self.attention = SelfAttention(in_channels=512, out_channels=512)\n        \n        # Fully connected layer\n        self.fc = nn.Linear(512 * 15 * 15, 124)  # Adjust the input size based on the output size of the last convolutional layer\n        \n        # Xavier Initialization\n        self.initialize_weights()\n\n    def forward(self, x):\n        # Convolutional layers with ReLU activation and maxpooling\n        x = F.relu(self.conv1(x))\n        x = self.maxpool(x)\n        x = F.relu(self.conv2(x))\n        x = self.maxpool(x)\n        x = F.relu(self.conv3(x))\n        x = self.maxpool(x)\n        x = F.relu(self.conv4(x))\n        x = self.maxpool(x)\n        \n        x = self.attention(x)\n        \n        # Flatten the output for fully connected layers\n        x = x.view(-1, 512 * 15 * 15)\n        \n        # Fully connected layer\n        x = self.fc(x)\n        \n        return x\n    \n    def initialize_weights(self):\n        # Xavier Initialization for convolutional layers\n        nn.init.xavier_uniform_(self.conv1.weight)\n        nn.init.xavier_uniform_(self.conv2.weight)\n        nn.init.xavier_uniform_(self.conv3.weight)\n        nn.init.xavier_uniform_(self.conv4.weight)\n        \n        # Xavier Initialization for fully connected layer\n        nn.init.xavier_uniform_(self.fc.weight)\n\n# Load the trained model\nmodel_path = '/kaggle/input/weight-files/cnn_attention_1.pth'\nmodel_state = torch.load(model_path, map_location=torch.device('cpu'))  # Load model parameters\nmodel = CNNModel()  # Initialize your model\nmodel.load_state_dict(model_state, strict=False)   # Load model parameters (allowing for mismatch)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-19T05:57:31.993168Z","iopub.execute_input":"2024-03-19T05:57:31.993543Z","iopub.status.idle":"2024-03-19T05:57:33.009428Z","shell.execute_reply.started":"2024-03-19T05:57:31.993508Z","shell.execute_reply":"2024-03-19T05:57:33.008474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cuda = torch.cuda.is_available()\n\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom torchvision import transforms\n\n\narr=[1,1,10,100,101,102,103,104,105,106,107,108,109,\n     11,110,111,112,113,114,115,116,117,118,119,\n     12,121,122,123,124,\n    13,14,15,16,17,18,19,\n    2,20,21,22,23,24,25,26,27,28,29,\n    3,30,31,32,33,34,35,36,37,38,39,\n    4,40,41,42,43,44,45,46,47,48,49,\n    5,50,51,52,53,54,55,56,57,58,59,\n    6,60,61,62,63,64,65,66,67,68,69,\n    7,70,71,72,73,74,75,76,77,78,79,\n    8,80,81,82,83,84,85,86,87,88,89,\n    9,90,91,92,93,94,95,96,97,98,99,]\n\ntransform = transforms.Compose([\n    transforms.Grayscale(num_output_channels=1),  \n    transforms.Resize((240, 240)),               \n    transforms.ToTensor(),                       \n    transforms.Normalize(mean=[0.5], std=[0.5])\n])\n\n\nimages=os.listdir('/kaggle/working/data/testA')\n\ntotal_correct=0\ntotal_images=0\n\nG_AB.cuda()\nG_AB.eval()\n    \ntruth=[]\npredicted=[]\n    \nfor img in images:\n    \n    image_path = '/kaggle/working/data/testA/'+img\n    input_image = Image.open(image_path)    \n    input_tensor = transform(input_image)\n    input_tensor = input_tensor.unsqueeze(0)\n    if cuda:\n        input_tensor = input_tensor.cuda()\n        model.cuda()\n    \n    model.eval()\n    \n    with torch.no_grad():\n        generated_image = G_AB(input_tensor)\n    \n#     real_A = make_grid(input_tensor, nrow=1, normalize=True)\n#     fake_B = make_grid(generated_image, nrow=1, normalize=True)\n    \n\n#     image_grid = torch.cat((real_A, fake_B), -1)\n    \n#     image_grid_np = image_grid.permute(1, 2, 0).cpu().detach().numpy()\n    \n#     plt.imshow(image_grid_np)\n#     plt.axis('off')\n#     plt.show()\n\n    with torch.no_grad():\n        output = model(generated_image)\n#         output=model(input_tensor)\n\n\n    _, predicted_class = torch.max(output, 1)\n    predicted_class = predicted_class.item()\n    \n    ground_truth = int(img[:3])\n    \n    truth.append(ground_truth)\n    predicted.append(predicted_class+1)\n    \n    \n\n#     print(img[:3], \" - \",predicted_class+1)\n\n#     if ground_truth == predicted_class+1:\n#         total_correct += 1\n    \n#     total_images += 1\n\n# precision = total_correct / total_images\n# print(\"Precision:\", precision)\n\n# print(truth,predicted)\n\n# Compute accuracy\naccuracy = accuracy_score(truth, predicted)\n\n# Compute precision\nprecision = precision_score(truth, predicted, average='weighted')\n\n# Compute recall\nrecall = recall_score(truth, predicted, average='weighted')\n\n# Compute F1 score\nf1 = f1_score(truth, predicted, average='weighted')\n\nprint(\"Accuracy:\", accuracy)\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\nprint(\"F1 Score:\", f1)","metadata":{"execution":{"iopub.status.busy":"2024-03-19T06:00:10.603548Z","iopub.execute_input":"2024-03-19T06:00:10.603942Z","iopub.status.idle":"2024-03-19T06:00:47.586655Z","shell.execute_reply.started":"2024-03-19T06:00:10.603904Z","shell.execute_reply":"2024-03-19T06:00:47.585696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = go.Figure()\nfig.add_trace(go.Scatter(x=train_counter, y=train_losses_gen, mode='lines', name='Train Gen Loss (Loss_G)'))\nfig.add_trace(go.Scatter(x=train_counter, y=train_losses_id, mode='lines', name='Train Gen Identity Loss'))\nfig.add_trace(go.Scatter(x=train_counter, y=train_losses_gan, mode='lines', name='Train Gen GAN Loss'))\nfig.add_trace(go.Scatter(x=train_counter, y=train_losses_cyc, mode='lines', name='Train Gen Cyclic Loss'))\nfig.add_trace(go.Scatter(x=test_counter, y=test_losses_gen, marker_symbol='star-diamond', \n                         marker_color='orange', marker_line_width=1, marker_size=9, mode='markers', name='Test Gen Loss (Loss_G)'))\nfig.update_layout(\n    width=1000,\n    height=500,\n    title=\"Train vs. Test Generator Loss\",\n    xaxis_title=\"Number of training examples seen (A+B)\",\n    yaxis_title=\"Generator Losses\"),\nplotly.offline.plot(fig, filename = 'plotly_gen_losses.html')\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import shutil\nimport os\n\n# Specify the path to the working directory\nworking_directory = \"/kaggle/working/saved_models\"\n\n# Iterate over the files in the working directory and remove them\nfor filename in os.listdir(working_directory):\n    file_path = os.path.join(working_directory, filename)\n    try:\n        if os.path.isfile(file_path) or os.path.islink(file_path):\n            os.unlink(file_path)\n        elif os.path.isdir(file_path):\n            shutil.rmtree(file_path)\n    except Exception as e:\n        print(f\"Failed to delete {file_path}: {e}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}