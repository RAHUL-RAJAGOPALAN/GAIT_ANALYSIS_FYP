{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":3394100,"sourceType":"datasetVersion","datasetId":2042641},{"sourceId":7852954,"sourceType":"datasetVersion","datasetId":4605651},{"sourceId":7867762,"sourceType":"datasetVersion","datasetId":4616149}],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport shutil\nimport numpy as np\nimport pandas as pd\npath=\"/kaggle/working/\"\nos.makedirs(os.path.join(path,'data'),exist_ok=True)\n\npath='/kaggle/working/data'\n# os.makedirs(os.path.join(path,'trainA'),exist_ok=True)\n# os.makedirs(os.path.join(path,'trainB'),exist_ok=True)\n# os.makedirs(os.path.join(path,'train_cloth'),exist_ok=True)\nos.makedirs(os.path.join(path,'train_bag'),exist_ok=True)\n\n\ndef organize_folders(root_folder, dest_folder):\n    for split_folder in ['test', 'train', 'val']:\n        split_path = os.path.join(root_folder, split_folder)\n\n        for subfolder in os.listdir(split_path):\n            subfolder_path = os.path.join(split_path, subfolder)\n            dest_path = os.path.join(dest_folder, subfolder)\n\n            for image_file in os.listdir(subfolder_path):\n                source_path = os.path.join(subfolder_path, image_file)\n\n                if 'bg' in image_file:\n                    destination_folder = 'train_bag'\n                    \n                elif 'cl' in image_file:\n#                     destination_folder = 'train_cloth'\n                    continue\n                elif 'nm' in image_file:\n#                     destination_folder = 'trainB'\n                    continue\n#                 print(image_file)\n                # Constructing the destination path using os.path.join\n                destination_path = os.path.join(dest_folder, destination_folder, os.path.basename(image_file))\n                shutil.copy(source_path, destination_path)\n\n# Example usage\nroot_folder = \"/kaggle/input/casiab-identification/DATASETidentification\"\ndest_folder = \"/kaggle/working/data/\"\norganize_folders(root_folder, dest_folder)\n\n\n# splitfolders.ratio(input_folder, output=output_folder, seed=1337, ratio=ratio, group_prefix=None)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-19T03:56:29.661785Z","iopub.execute_input":"2024-03-19T03:56:29.662694Z","iopub.status.idle":"2024-03-19T03:56:46.653499Z","shell.execute_reply.started":"2024-03-19T03:56:29.662661Z","shell.execute_reply":"2024-03-19T03:56:46.652420Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"import shutil\nimport random\n\n\nos.makedirs(\"/kaggle/working/data/testA\", exist_ok=True)\nos.makedirs(\"/kaggle/working/data/testB\", exist_ok=True)\n\nimages=os.listdir(\"/kaggle/working/data/trainA\")\n# img=random.sample(images,815)\nimg=random.sample(images,1631)\n\nfor image in img:\n    src = os.path.join(\"/kaggle/working/data/trainA\", image)\n    dst = os.path.join(\"/kaggle/working/data/testA\", image)\n    shutil.move(src, dst)\n\n\nimages=os.listdir(\"/kaggle/working/data/trainB\")\nimg=random.sample(images,2446)\nfor image in img:\n    src = os.path.join(\"/kaggle/working/data/trainB\", image)\n    dst = os.path.join(\"/kaggle/working/data/testB\", image)\n    shutil.move(src, dst)\n\n\n     ","metadata":{"execution":{"iopub.status.busy":"2024-03-17T14:33:28.935006Z","iopub.execute_input":"2024-03-17T14:33:28.935522Z","iopub.status.idle":"2024-03-17T14:33:29.087627Z","shell.execute_reply.started":"2024-03-17T14:33:28.935489Z","shell.execute_reply":"2024-03-17T14:33:29.086703Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import os\nimport shutil\n\ndef copy_images_with_condition(src_folder, dest_folder, valid_extensions=(\".jpg\", \".jpeg\", \".png\"), substring=\"nm\"):\n    \"\"\"\n    Copy image files from the source folder to the destination folder based on specified conditions.\n\n    Parameters:\n    - src_folder (str): Path to the source folder containing image files.\n    - dest_folder (str): Path to the destination folder where images will be copied.\n    - valid_extensions (tuple): Tuple of valid image file extensions. Default is (\".jpg\", \".jpeg\", \".png\").\n    - substring (str): Substring to look for in the image filenames. Default is \"nm\".\n    \"\"\"\n    # Iterate through each file in the source folder\n    for file_name in os.listdir(src_folder):\n        src_file_path = os.path.join(src_folder, file_name)\n\n        # Check if the file is a regular file and has a valid image extension\n        if os.path.isfile(src_file_path) and file_name.lower().endswith(valid_extensions):\n            # Check if the image filename contains the specified substring\n            if substring in file_name:\n                # Create the destination folder if it doesn't exist\n                os.makedirs(dest_folder, exist_ok=True)\n\n                # Construct the destination file path\n                dest_file_path = os.path.join(dest_folder, file_name)\n\n                # Copy the image to the destination folder\n                shutil.copy(src_file_path, dest_file_path)\n                # If you want to move the image, use shutil.move instead of shutil.copy\n\n\n# Example usage:\n\nfor ele in ['train', 'test', 'val']:\n    dest_folder_path = f\"/kaggle/working/inp/{ele}\" #change final : 'val, test, train'\n\n    for subject_number in range(1, 125):\n        src_folder_path = f\"/kaggle/input/casiab-identification/DATASETidentification/{ele}/{subject_number}\" #change final : 'val, test, train'\n        copy_images_with_condition(src_folder_path, dest_folder_path)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-19T03:05:25.371931Z","iopub.execute_input":"2024-03-19T03:05:25.372405Z","iopub.status.idle":"2024-03-19T03:06:06.753912Z","shell.execute_reply.started":"2024-03-19T03:05:25.372375Z","shell.execute_reply":"2024-03-19T03:06:06.753099Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"LABELLING THE IMAGES","metadata":{}},{"cell_type":"code","source":"import os\nimport shutil\n\ndef group_images_by_label(source_folder):\n    \"\"\"\n    Group images by label and move them to corresponding subfolders.\n\n    Parameters:\n    - source_folder (str): Path to the folder containing images.\n    \"\"\"\n    # Iterate through each image in the source folder\n    for file_name in os.listdir(source_folder):\n        if file_name.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n            # Extract label from the first 3 characters of the filename\n            label = file_name[:3]\n\n            # Create subfolder if it doesn't exist\n            subfolder_path = os.path.join(source_folder, label)\n            os.makedirs(subfolder_path, exist_ok=True)\n\n            # Move the image to the corresponding subfolder\n            source_file_path = os.path.join(source_folder, file_name)\n            destination_file_path = os.path.join(subfolder_path, file_name)\n            shutil.move(source_file_path, destination_file_path)\n\n    print(\"Images grouped by label.\")\n\n# Example usage:\nfor ele in ['train', 'val']:\n    source_folder_path = f\"/kaggle/working/inp/{ele}\"\n    group_images_by_label(source_folder_path)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-19T03:08:06.722743Z","iopub.execute_input":"2024-03-19T03:08:06.723441Z","iopub.status.idle":"2024-03-19T03:08:07.011642Z","shell.execute_reply.started":"2024-03-19T03:08:06.723410Z","shell.execute_reply":"2024-03-19T03:08:07.010781Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Images grouped by label.\nImages grouped by label.\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport shutil\n\n# Define the paths to the two directories and the combined directory\ndir1_path = '/kaggle/working/inp/train'\ndir2_path = '/kaggle/working/inp/val'\ncombined_dir_path = '/kaggle/working/inp/TRAIN'\n\n# Ensure the combined directory exists\nif not os.path.exists(combined_dir_path):\n    os.makedirs(combined_dir_path)\n\n# Traverse each directory and copy images to the combined directory\nfor folder_name in os.listdir(dir1_path):\n    folder1_path = os.path.join(dir1_path, folder_name)\n    folder2_path = os.path.join(dir2_path, folder_name)\n    combined_folder_path = os.path.join(combined_dir_path, folder_name)\n\n    # Ensure the combined subfolder exists\n    if not os.path.exists(combined_folder_path):\n        os.makedirs(combined_folder_path)\n\n    # Copy images from directory 1\n    for image_name in os.listdir(folder1_path):\n        image_path = os.path.join(folder1_path, image_name)\n        shutil.copy(image_path, combined_folder_path)\n\n    # Copy images from directory 2\n    for image_name in os.listdir(folder2_path):\n        image_path = os.path.join(folder2_path, image_name)\n        shutil.copy(image_path, combined_folder_path)\n\nprint('Images copied successfully to the combined directory.')\n","metadata":{"execution":{"iopub.status.busy":"2024-03-19T03:08:48.256547Z","iopub.execute_input":"2024-03-19T03:08:48.256915Z","iopub.status.idle":"2024-03-19T03:08:48.892684Z","shell.execute_reply.started":"2024-03-19T03:08:48.256885Z","shell.execute_reply":"2024-03-19T03:08:48.891617Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Images copied successfully to the combined directory.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"***CNN***","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import transforms, datasets\nfrom torch.utils.data import DataLoader\n\nclass SelfAttention(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(SelfAttention, self).__init__()\n        self.query = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n        self.key = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n        self.value = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n        self.softmax = nn.Softmax(dim=-1)\n\n    def forward(self, x):\n        query = self.query(x)\n        key = self.key(x)\n        value = self.value(x)\n\n        energy = torch.matmul(query, key.transpose(2, 3))  # Calculate energy scores\n        attention_weights = self.softmax(energy)  # Apply softmax to get attention weights\n        attended_values = torch.matmul(attention_weights, value)  # Apply attention weights to values\n\n        return attended_values\n\n\nclass CNNModel(nn.Module):\n    def __init__(self):\n        super(CNNModel, self).__init__()\n        # Convolutional layers\n        self.conv1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, stride=1, padding=1)\n        self.conv2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n        self.conv3 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1)\n        self.conv4 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1)\n        \n        # Maxpool layers\n        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n        \n        self.attention = SelfAttention(in_channels=512, out_channels=512)\n        \n        # Fully connected layer\n        self.fc = nn.Linear(512 * 15 * 15, 124)  # Adjust the input size based on the output size of the last convolutional layer\n        \n        # Xavier Initialization\n        self.initialize_weights()\n\n    def forward(self, x):\n        # Convolutional layers with ReLU activation and maxpooling\n        x = F.relu(self.conv1(x))\n        x = self.maxpool(x)\n        x = F.relu(self.conv2(x))\n        x = self.maxpool(x)\n        x = F.relu(self.conv3(x))\n        x = self.maxpool(x)\n        x = F.relu(self.conv4(x))\n        x = self.maxpool(x)\n        \n        x = self.attention(x)\n        \n        # Flatten the output for fully connected layers\n        x = x.view(-1, 512 * 15 * 15)\n        \n        # Fully connected layer\n        x = self.fc(x)\n        \n        return x\n    \n    def initialize_weights(self):\n        # Xavier Initialization for convolutional layers\n        nn.init.xavier_uniform_(self.conv1.weight)\n        nn.init.xavier_uniform_(self.conv2.weight)\n        nn.init.xavier_uniform_(self.conv3.weight)\n        nn.init.xavier_uniform_(self.conv4.weight)\n        \n        # Xavier Initialization for fully connected layer\n        nn.init.xavier_uniform_(self.fc.weight)\n\n# Example usage:\nroot_dir = '/kaggle/working/inp/TRAIN'\ntransform = transforms.Compose([\n    transforms.Resize((240, 240)),  # Resize images to 240x240 (adjust size as needed)\n    transforms.Grayscale(num_output_channels=1),  # Convert images to grayscale (ensure one channel)\n    transforms.ToTensor(),           # Convert images to PyTorch tensors\n    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize image tensors (assuming grayscale)\n])\ndataset = datasets.ImageFolder(root=root_dir, transform=transform)\n\n# Create a DataLoader for your dataset\nbatch_size = 20  # Adjust batch size as needed\nshuffle = True   # Shuffle data during training\ndata_loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n\n# Initialize your CNN model\nmodel = CNNModel()\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)  # Move model to appropriate device\n\n# Define loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.0001)\n\n# Training loop\nnum_epochs = 50\nfor epoch in range(num_epochs):\n    model.train()  # Set model to training mode\n    running_loss = 0.0\n    \n    for inputs, labels in data_loader:\n        inputs, labels = inputs.to(device), labels.to(device)  # Move data to device\n        optimizer.zero_grad()      # Zero the parameter gradients\n        \n        outputs = model(inputs)    # Forward pass\n        loss = criterion(outputs, labels)  # Compute the loss\n        loss.backward()            # Backward pass\n        optimizer.step()           # Update weights\n        \n        running_loss += loss.item() * inputs.size(0)\n    \n    epoch_loss = running_loss / len(dataset)\n    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}')\n\nmodel_path = '/kaggle/working/cnn_attention_1.pth'\ntorch.save(model.state_dict(), model_path)","metadata":{"execution":{"iopub.status.busy":"2024-03-19T03:10:56.323705Z","iopub.execute_input":"2024-03-19T03:10:56.324081Z","iopub.status.idle":"2024-03-19T03:45:10.777742Z","shell.execute_reply.started":"2024-03-19T03:10:56.324049Z","shell.execute_reply":"2024-03-19T03:45:10.776711Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Epoch [1/50], Loss: 4.8466\nEpoch [2/50], Loss: 4.7477\nEpoch [3/50], Loss: 4.1138\nEpoch [4/50], Loss: 2.2441\nEpoch [5/50], Loss: 0.7174\nEpoch [6/50], Loss: 0.2067\nEpoch [7/50], Loss: 0.0901\nEpoch [8/50], Loss: 0.0515\nEpoch [9/50], Loss: 0.0776\nEpoch [10/50], Loss: 0.0743\nEpoch [11/50], Loss: 0.0152\nEpoch [12/50], Loss: 0.0432\nEpoch [13/50], Loss: 0.0549\nEpoch [14/50], Loss: 0.0323\nEpoch [15/50], Loss: 0.0418\nEpoch [16/50], Loss: 0.0205\nEpoch [17/50], Loss: 0.0229\nEpoch [18/50], Loss: 0.0525\nEpoch [19/50], Loss: 0.0515\nEpoch [20/50], Loss: 0.0429\nEpoch [21/50], Loss: 0.0356\nEpoch [22/50], Loss: 0.0054\nEpoch [23/50], Loss: 0.0002\nEpoch [24/50], Loss: 0.0001\nEpoch [25/50], Loss: 0.0001\nEpoch [26/50], Loss: 0.0000\nEpoch [27/50], Loss: 0.0000\nEpoch [28/50], Loss: 0.0000\nEpoch [29/50], Loss: 0.0000\nEpoch [30/50], Loss: 0.0000\nEpoch [31/50], Loss: 0.0000\nEpoch [32/50], Loss: 0.0000\nEpoch [33/50], Loss: 0.0000\nEpoch [34/50], Loss: 0.0000\nEpoch [35/50], Loss: 0.0000\nEpoch [36/50], Loss: 0.0000\nEpoch [37/50], Loss: 0.0000\nEpoch [38/50], Loss: 0.0000\nEpoch [39/50], Loss: 0.0000\nEpoch [40/50], Loss: 0.0000\nEpoch [41/50], Loss: 0.0000\nEpoch [42/50], Loss: 0.0000\nEpoch [43/50], Loss: 0.0000\nEpoch [44/50], Loss: 0.0000\nEpoch [45/50], Loss: 0.0000\nEpoch [46/50], Loss: 0.0000\nEpoch [47/50], Loss: 0.0000\nEpoch [48/50], Loss: 0.0000\nEpoch [49/50], Loss: 0.0000\nEpoch [50/50], Loss: 0.0000\n","output_type":"stream"}]},{"cell_type":"markdown","source":"***TESTING THE TRAINED CNN MODEL***","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import transforms, datasets\nfrom torch.utils.data import DataLoader\nfrom PIL import Image\n\nclass SelfAttention(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(SelfAttention, self).__init__()\n        self.query = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n        self.key = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n        self.value = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n        self.softmax = nn.Softmax(dim=-1)\n\n    def forward(self, x):\n        query = self.query(x)\n        key = self.key(x)\n        value = self.value(x)\n\n        energy = torch.matmul(query, key.transpose(2, 3))  # Calculate energy scores\n        attention_weights = self.softmax(energy)  # Apply softmax to get attention weights\n        attended_values = torch.matmul(attention_weights, value)  # Apply attention weights to values\n\n        return attended_values\n\n\nclass CNNModel(nn.Module):\n    def __init__(self):\n        super(CNNModel, self).__init__()\n        # Convolutional layers\n        self.conv1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, stride=1, padding=1)\n        self.conv2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n        self.conv3 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1)\n        self.conv4 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1)\n        \n        # Maxpool layers\n        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n        \n        self.attention = SelfAttention(in_channels=512, out_channels=512)\n        \n        # Fully connected layer\n        self.fc = nn.Linear(512 * 15 * 15, 124)  # Adjust the input size based on the output size of the last convolutional layer\n        \n        # Xavier Initialization\n        self.initialize_weights()\n\n    def forward(self, x):\n        # Convolutional layers with ReLU activation and maxpooling\n        x = F.relu(self.conv1(x))\n        x = self.maxpool(x)\n        x = F.relu(self.conv2(x))\n        x = self.maxpool(x)\n        x = F.relu(self.conv3(x))\n        x = self.maxpool(x)\n        x = F.relu(self.conv4(x))\n        x = self.maxpool(x)\n        \n        x = self.attention(x)\n        \n        # Flatten the output for fully connected layers\n        x = x.view(-1, 512 * 15 * 15)\n        \n        # Fully connected layer\n        x = self.fc(x)\n        \n        return x\n    \n    def initialize_weights(self):\n        # Xavier Initialization for convolutional layers\n        nn.init.xavier_uniform_(self.conv1.weight)\n        nn.init.xavier_uniform_(self.conv2.weight)\n        nn.init.xavier_uniform_(self.conv3.weight)\n        nn.init.xavier_uniform_(self.conv4.weight)\n        \n        # Xavier Initialization for fully connected layer\n        nn.init.xavier_uniform_(self.fc.weight)\n\n# Load the trained model\nmodel_path = '/kaggle/working/cnn_attention_1.pth'\nmodel_state = torch.load(model_path, map_location=torch.device('cpu'))  # Load model parameters\nmodel = CNNModel()  # Initialize your model\nmodel.load_state_dict(model_state, strict=False)   # Load model parameters (allowing for mismatch)\n\n# Define the transformation for input images\n\n","metadata":{"execution":{"iopub.status.busy":"2024-03-19T03:47:08.536887Z","iopub.execute_input":"2024-03-19T03:47:08.537249Z","iopub.status.idle":"2024-03-19T03:47:08.892962Z","shell.execute_reply.started":"2024-03-19T03:47:08.537206Z","shell.execute_reply":"2024-03-19T03:47:08.892029Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"code","source":"cuda = torch.cuda.is_available()\n\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom torchvision import transforms\n\n\narr=[1,1,10,100,101,102,103,104,105,106,107,108,109,\n     11,110,111,112,113,114,115,116,117,118,119,\n     12,121,122,123,124,\n    13,14,15,16,17,18,19,\n    2,20,21,22,23,24,25,26,27,28,29,\n    3,30,31,32,33,34,35,36,37,38,39,\n    4,40,41,42,43,44,45,46,47,48,49,\n    5,50,51,52,53,54,55,56,57,58,59,\n    6,60,61,62,63,64,65,66,67,68,69,\n    7,70,71,72,73,74,75,76,77,78,79,\n    8,80,81,82,83,84,85,86,87,88,89,\n    9,90,91,92,93,94,95,96,97,98,99,]\n\ntransform = transforms.Compose([\n    transforms.Grayscale(num_output_channels=1),  \n    transforms.Resize((240, 240)),               \n    transforms.ToTensor(),                       \n    transforms.Normalize(mean=[0.5], std=[0.5])\n])\n\n\nimages=os.listdir('/kaggle/working/data/train_bag')\n\ntotal_correct=0\ntotal_images=0\n\n# G_AB.cuda()\n# G_AB.eval()\n    \ntruth=[]\npredicted=[]\n    \nfor img in images:\n    \n    image_path = '/kaggle/working/data/train_bag/'+img\n    input_image = Image.open(image_path)    \n    input_tensor = transform(input_image)\n    input_tensor = input_tensor.unsqueeze(0)\n    if cuda:\n        input_tensor = input_tensor.cuda()\n        model.cuda()\n    \n    model.eval()\n    \n#     with torch.no_grad():\n#         generated_image = G_AB(input_tensor)\n    \n#     real_A = make_grid(input_tensor, nrow=1, normalize=True)\n#     fake_B = make_grid(generated_image, nrow=1, normalize=True)\n    \n\n#     image_grid = torch.cat((real_A, fake_B), -1)\n    \n#     image_grid_np = image_grid.permute(1, 2, 0).cpu().detach().numpy()\n    \n#     plt.imshow(image_grid_np)\n#     plt.axis('off')\n#     plt.show()\n\n    with torch.no_grad():\n#         output = model(generated_image)\n        output=model(input_tensor)\n\n\n    _, predicted_class = torch.max(output, 1)\n    predicted_class = predicted_class.item()\n    \n    ground_truth = int(img[:3])\n    \n    truth.append(ground_truth)\n    predicted.append(predicted_class+1)\n    \n    \n\n#     print(img[:3], \" - \",predicted_class+1)\n\n#     if ground_truth == predicted_class+1:\n#         total_correct += 1\n    \n#     total_images += 1\n\n# precision = total_correct / total_images\n# print(\"Precision:\", precision)\n\n# print(truth,predicted)\n\n# Compute accuracy\naccuracy = accuracy_score(truth, predicted)\n\n# Compute precision\nprecision = precision_score(truth, predicted, average='weighted')\n\n# Compute recall\nrecall = recall_score(truth, predicted, average='weighted')\n\n# Compute F1 score\nf1 = f1_score(truth, predicted, average='weighted')\n\nprint(\"Accuracy:\", accuracy)\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\nprint(\"F1 Score:\", f1)","metadata":{"execution":{"iopub.status.busy":"2024-03-19T03:59:08.299761Z","iopub.execute_input":"2024-03-19T03:59:08.300548Z","iopub.status.idle":"2024-03-19T03:59:18.131180Z","shell.execute_reply.started":"2024-03-19T03:59:08.300516Z","shell.execute_reply":"2024-03-19T03:59:18.130321Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Accuracy: 0.5493009565857248\nPrecision: 0.63830151715355\nRecall: 0.5493009565857248\nF1 Score: 0.5401301273225716\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch.nn.functional as F\n\nclass EnhancedCNNModel(nn.Module):\n    def _init_(self):\n        super(EnhancedCNNModel, self)._init_()\n        \n        # Convolutional layers\n        self.conv1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, stride=1, padding=1)\n        self.conv2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n        self.conv3 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1)\n        self.conv4 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1)\n        self.conv5 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1)\n        \n        # Maxpool layers\n        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n        \n        # Fully connected layers\n        self.fc1 = nn.Linear(512 * 7 * 7, 1024)\n        self.fc2 = nn.Linear(1024, 512)\n        self.fc3 = nn.Linear(512, 124)  # Adjust the output size based on the number of classes\n        \n        # Batch normalization\n        self.batch_norm1 = nn.BatchNorm2d(64)\n        self.batch_norm2 = nn.BatchNorm2d(128)\n        self.batch_norm3 = nn.BatchNorm2d(256)\n        self.batch_norm4 = nn.BatchNorm2d(512)\n        \n        # Dropout\n        self.dropout = nn.Dropout(0.5)\n        \n        # Xavier Initialization\n        self.initialize_weights()\n\n    def forward(self, x):\n        # Convolutional layers with ReLU activation and maxpooling\n        x = F.relu(self.conv1(x))\n        x = self.batch_norm1(x)\n        x = self.maxpool(x)\n        \n        x = F.relu(self.conv2(x))\n        x = self.batch_norm2(x)\n        x = self.maxpool(x)\n        \n        x = F.relu(self.conv3(x))\n        x = self.batch_norm3(x)\n        x = self.maxpool(x)\n        \n        x = F.relu(self.conv4(x))\n        x = self.batch_norm4(x)\n        x = self.maxpool(x)\n        \n        x = F.relu(self.conv5(x))\n        x = self.batch_norm4(x)\n        x = self.maxpool(x)\n        \n        # Flatten the output for fully connected layers\n        x = x.view(-1, 512 * 7 * 7)\n        \n        # Fully connected layers with dropout\n        x = F.relu(self.fc1(x))\n        x = self.dropout(x)\n        \n        x = F.relu(self.fc2(x))\n        x = self.dropout(x)\n        \n        # Output layer\n        x = self.fc3(x)\n        \n        return x\n    \n    def initialize_weights(self):\n        # Xavier Initialization for convolutional layers\n        nn.init.xavier_uniform_(self.conv1.weight)\n        nn.init.xavier_uniform_(self.conv2.weight)\n        nn.init.xavier_uniform_(self.conv3.weight)\n        nn.init.xavier_uniform_(self.conv4.weight)\n        nn.init.xavier_uniform_(self.conv5.weight)\n        \n        # Xavier Initialization for fully connected layers\n        nn.init.xavier_uniform_(self.fc1.weight)\n        nn.init.xavier_uniform_(self.fc2.weight)\n        nn.init.xavier_uniform_(self.fc3.weight)\n\n        \n# Example usage:\nroot_dir = '/kaggle/working/inp/train'\ntransform = transforms.Compose([\n    transforms.Resize((240, 240)),  # Resize images to 240x240 (adjust size as needed)\n    transforms.Grayscale(num_output_channels=1),  # Convert images to grayscale (ensure one channel)\n    transforms.ToTensor(),           # Convert images to PyTorch tensors\n    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize image tensors (assuming grayscale)\n])\ndataset = datasets.ImageFolder(root=root_dir, transform=transform)\n\n# Create a DataLoader for your dataset\nbatch_size = 64  # Adjust batch size as needed\nshuffle = True   # Shuffle data during training\ndata_loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n\n# Initialize your CNN model\nmodel = CNNModel()\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)  # Move model to appropriate device\n\n# Define loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.01)\n\n# Training loop\nnum_epochs = 10\nfor epoch in range(num_epochs):\n    model.train()  # Set model to training mode\n    running_loss = 0.0\n    \n    for inputs, labels in data_loader:\n        inputs, labels = inputs.to(device), labels.to(device)  # Move data to device\n        optimizer.zero_grad()      # Zero the parameter gradients\n        \n        outputs = model(inputs)    # Forward pass\n        loss = criterion(outputs, labels)  # Compute the loss\n        loss.backward()            # Backward pass\n        optimizer.step()           # Update weights\n        \n        running_loss += loss.item() * inputs.size(0)\n    \n    epoch_loss = running_loss / len(dataset)\n    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}')\n\nmodel_path = '/kaggle/working/cnn2.pth'\ntorch.save(model.state_dict(), model_path)\n        ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***ATTENTION CNN NEW CODE***","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass SelfAttention(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(SelfAttention, self).__init__()\n        self.query = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n        self.key = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n        self.value = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n        self.softmax = nn.Softmax(dim=-1)\n\n    def forward(self, x):\n        query = self.query(x)\n        key = self.key(x)\n        value = self.value(x)\n\n        energy = torch.matmul(query, key.transpose(2, 3))  # Calculate energy scores\n        attention_weights = self.softmax(energy)  # Apply softmax to get attention weights\n        attended_values = torch.matmul(attention_weights, value)  # Apply attention weights to values\n\n        return attended_values\n\nclass AttentionGuidedDnCNN(nn.Module):\n    def __init__(self, channels, num_of_layers=15):\n        super(AttentionGuidedDnCNN, self).__init__()\n        kernel_size = 3\n        padding = 1\n        features = 64\n        groups = 1\n\n        # Define the first convolutional layer and the attention module\n        self.conv1_1 = nn.Sequential(\n            nn.Conv2d(in_channels=channels, out_channels=features, kernel_size=kernel_size, padding=padding, groups=groups, bias=False),\n            nn.BatchNorm2d(features),\n            nn.ReLU(inplace=True)\n        )\n        self.attention = SelfAttention(features, features)  # Attention module with input and output channels as 'features'\n\n        # Define other convolutional layers\n        self.conv1_2 = nn.Sequential(\n            nn.Conv2d(in_channels=features, out_channels=features, kernel_size=kernel_size, padding=2, groups=groups, bias=False, dilation=2),\n            nn.BatchNorm2d(features),\n            nn.ReLU(inplace=True)\n        )\n        self.conv1_3 = nn.Sequential(\n            nn.Conv2d(in_channels=features, out_channels=features, kernel_size=kernel_size, padding=1, groups=groups, bias=False),\n            nn.BatchNorm2d(features),\n            nn.ReLU(inplace=True)\n        )\n        self.conv1_4 = nn.Sequential(\n            nn.Conv2d(in_channels=features, out_channels=features, kernel_size=kernel_size, padding=1, groups=groups, bias=False),\n            nn.BatchNorm2d(features),\n            nn.ReLU(inplace=True)\n        )\n        self.conv1_5 = nn.Sequential(\n            nn.Conv2d(in_channels=features, out_channels=features, kernel_size=kernel_size, padding=2, groups=groups, bias=False, dilation=2),\n            nn.BatchNorm2d(features),\n            nn.ReLU(inplace=True)\n        )\n        self.conv1_6 = nn.Sequential(\n            nn.Conv2d(in_channels=features, out_channels=features, kernel_size=kernel_size, padding=1, groups=groups, bias=False),\n            nn.BatchNorm2d(features),\n            nn.ReLU(inplace=True)\n        )\n        self.conv1_7 = nn.Sequential(\n            nn.Conv2d(in_channels=features, out_channels=features, kernel_size=kernel_size, padding=padding, groups=groups, bias=False),\n            nn.BatchNorm2d(features),\n            nn.ReLU(inplace=True)\n        )\n        self.conv1_8 = nn.Sequential(\n            nn.Conv2d(in_channels=features, out_channels=features, kernel_size=kernel_size, padding=1, groups=groups, bias=False),\n            nn.BatchNorm2d(features),\n            nn.ReLU(inplace=True)\n        )\n        self.conv1_9 = nn.Sequential(\n            nn.Conv2d(in_channels=features, out_channels=features, kernel_size=kernel_size, padding=2, groups=groups, bias=False, dilation=2),\n            nn.BatchNorm2d(features),\n            nn.ReLU(inplace=True)\n        )\n        self.conv1_10 = nn.Sequential(\n            nn.Conv2d(in_channels=features, out_channels=features, kernel_size=kernel_size, padding=1, groups=groups, bias=False),\n            nn.BatchNorm2d(features),\n            nn.ReLU(inplace=True)\n        )\n        self.conv1_11 = nn.Sequential(\n            nn.Conv2d(in_channels=features, out_channels=features, kernel_size=kernel_size, padding=1, groups=groups, bias=False),\n            nn.BatchNorm2d(features),\n            nn.ReLU(inplace=True)\n        )\n        self.conv1_12 = nn.Sequential(\n            nn.Conv2d(in_channels=features, out_channels=features, kernel_size=kernel_size, padding=2, groups=groups, bias=False, dilation=2),\n            nn.BatchNorm2d(features),\n            nn.ReLU(inplace=True)\n        )\n        self.conv1_13 = nn.Sequential(\n            nn.Conv2d(in_channels=features, out_channels=features, kernel_size=kernel_size, padding=padding, groups=groups, bias=False),\n            nn.BatchNorm2d(features),\n            nn.ReLU(inplace=True)\n        )\n        self.conv1_14 = nn.Sequential(\n            nn.Conv2d(in_channels=features, out_channels=features, kernel_size=kernel_size, padding=padding, groups=groups, bias=False),\n            nn.BatchNorm2d(features),\n            nn.ReLU(inplace=True)\n        )\n        self.conv1_15 = nn.Sequential(\n            nn.Conv2d(in_channels=features, out_channels=features, kernel_size=kernel_size, padding=1, groups=groups, bias=False),\n            nn.BatchNorm2d(features),\n            nn.ReLU(inplace=True)\n        )\n        self.conv1_16 = nn.Conv2d(in_channels=features, out_channels=3, kernel_size=kernel_size, padding=1, groups=groups, bias=False)\n\n        # Final convolutional layer and activation functions\n        self.conv3 = nn.Conv2d(in_channels=2 * features, out_channels=3, kernel_size=1, stride=1, padding=0, groups=1, bias=True)\n        self.ReLU = nn.ReLU(inplace=True)\n        self.Tanh = nn.Tanh()\n        self.sigmoid = nn.Sigmoid()\n\n        # Weight initialization\n        self.initialize_weights()\n\n    def forward(self, x):\n        input = x\n        x1 = self.conv1_1(x)\n        x1 = self.attention(x1)\n        x1 = self.conv1_2(x1)\n        x1 = self.conv1_3(x1)\n        x1 = self.conv1_4(x1)\n        x1 = self.conv1_5(x1)\n        x1 = self.conv1_6(x1)\n        x1 = self.conv1_7(x1)\n        x1 = self.conv1_8(x1)\n        x1 = self.conv1_9(x1)\n        x1 = self.conv1_10(x1)\n        x1 = self.conv1_11(x1)\n        x1 = self.conv1_12(x1)\n        x1 = self.conv1_13(x1)\n        x1 = self.conv1_14(x1)\n        x1 = self.conv1_15(x1)\n        x1 = self.conv1_16(x1)\n        out = torch.cat([x, x1], 1)\n        out = self.Tanh(out)\n        out = self.conv3(out)\n        out = out * x1\n        out2 = x - out\n        return out2\n\n    def initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n                \n","metadata":{"execution":{"iopub.status.busy":"2024-03-19T05:39:57.693956Z","iopub.execute_input":"2024-03-19T05:39:57.694391Z","iopub.status.idle":"2024-03-19T05:39:57.732071Z","shell.execute_reply.started":"2024-03-19T05:39:57.694356Z","shell.execute_reply":"2024-03-19T05:39:57.730895Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\n# Adjust the input shape\nbatch_size = 20\nnum_channels = 128  # Match the expected input channels for self.conv3\nheight, width = 240, 240\n\n# Instantiate the model with the corrected input shape\nmodel = AttentionGuidedDnCNN(channels=num_channels)\n\n# Generate a random input tensor with the specified shape\nexample_input = torch.randn(batch_size, num_channels, height, width)\n\n# Forward pass to get the output and check the shape\noutput = model(example_input)\nprint('Output shape:', output.shape)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-19T05:40:00.573671Z","iopub.execute_input":"2024-03-19T05:40:00.574558Z","iopub.status.idle":"2024-03-19T05:40:20.350845Z","shell.execute_reply.started":"2024-03-19T05:40:00.574526Z","shell.execute_reply":"2024-03-19T05:40:20.349365Z"},"trusted":true},"execution_count":3,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[3], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m example_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(batch_size, num_channels, height, width)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Forward pass to get the output and check the shape\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOutput shape:\u001b[39m\u001b[38;5;124m'\u001b[39m, output\u001b[38;5;241m.\u001b[39mshape)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[2], line 143\u001b[0m, in \u001b[0;36mAttentionGuidedDnCNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    141\u001b[0m out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([x, x1], \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    142\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mTanh(out)\n\u001b[0;32m--> 143\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m out \u001b[38;5;241m=\u001b[39m out \u001b[38;5;241m*\u001b[39m x1\n\u001b[1;32m    145\u001b[0m out2 \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m-\u001b[39m out\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [3, 128, 1, 1], expected input[20, 131, 240, 240] to have 128 channels, but got 131 channels instead"],"ename":"RuntimeError","evalue":"Given groups=1, weight of size [3, 128, 1, 1], expected input[20, 131, 240, 240] to have 128 channels, but got 131 channels instead","output_type":"error"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader  # Assuming your Attention Guided CNN is defined in a separate file\n\n# Define the root directory and transformations for your dataset\nroot_dir = '/kaggle/working/inp/TRAIN'\ntransform = transforms.Compose([\n    transforms.Resize((240, 240)),            # Resize images to 240x240\n    transforms.Grayscale(num_output_channels=1),  # Convert images to grayscale\n    transforms.ToTensor(),                   # Convert images to PyTorch tensors\n    transforms.Normalize(mean=[0.5], std=[0.5])     # Normalize image tensors\n])\n\n# Create a dataset and DataLoader\ndataset = datasets.ImageFolder(root=root_dir, transform=transform)\nbatch_size = 20\nshuffle = True\ndata_loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n\n# Initialize your Attention Guided CNN model\nmodel = AttentionGuidedDnCNN(channels=1)  # Assuming input images are grayscale\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Define loss function and optimizer\ncriterion = nn.MSELoss()  # Assuming Mean Squared Error loss for image denoising\noptimizer = optim.Adam(model.parameters(), lr=0.0001)\n\n# Training loop\nnum_epochs = 5\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    \n    for inputs, _ in data_loader:  # Assuming labels are not needed for denoising task\n        inputs = inputs.to(device)\n        optimizer.zero_grad()\n        \n        outputs = model(inputs)\n        loss = criterion(outputs, inputs)  # Compare output to input for denoising task\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item() * inputs.size(0)\n    \n    epoch_loss = running_loss / len(dataset)\n    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}')\n\n# Save the trained model\nmodel_path = '/kaggle/working/cnn_attention_new.pth'\ntorch.save(model.state_dict(), model_path)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-19T05:19:40.937192Z","iopub.execute_input":"2024-03-19T05:19:40.937837Z","iopub.status.idle":"2024-03-19T05:19:42.071757Z","shell.execute_reply.started":"2024-03-19T05:19:40.937805Z","shell.execute_reply":"2024-03-19T05:19:42.070371Z"},"trusted":true},"execution_count":16,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[16], line 41\u001b[0m\n\u001b[1;32m     38\u001b[0m inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     39\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 41\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, inputs)  \u001b[38;5;66;03m# Compare output to input for denoising task\u001b[39;00m\n\u001b[1;32m     43\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[14], line 143\u001b[0m, in \u001b[0;36mAttentionGuidedDnCNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    141\u001b[0m out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([x, x1], \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    142\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mTanh(out)\n\u001b[0;32m--> 143\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m out \u001b[38;5;241m=\u001b[39m out \u001b[38;5;241m*\u001b[39m x1\n\u001b[1;32m    145\u001b[0m out2 \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m-\u001b[39m out\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [3, 128, 1, 1], expected input[20, 4, 240, 240] to have 128 channels, but got 4 channels instead"],"ename":"RuntimeError","evalue":"Given groups=1, weight of size [3, 128, 1, 1], expected input[20, 4, 240, 240] to have 128 channels, but got 4 channels instead","output_type":"error"}]}]}